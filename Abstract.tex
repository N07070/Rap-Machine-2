\documentclass[12pt,a4paper]{article}

\usepackage{graphicx}
\usepackage{url}


\title{Rap and Text generation\newline\ How a rap text can be generated considering the rhyming, metrical, and lexical questions}
\date{Octobre 2019}
\author{E. Partouche \and H. Hamila \and N. Perdriau}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Abstract}

In this paper, we will aim to show how we can use a Long Short-Term Memory Language (LSTM) to generate a rap text or to create a “rap machine”, including the generation of diverse recurrent requirements for this kind of texts such as rhymes, rhythm and a diverse vocabulary. \newline

A large part of the previous studies on the generation of text \cite{sutskever_generating_nodate} have as basis a Neural network, especially Recurrent Neural Networks (RNN): when a Neural Network is a circuit of artificial neurons made to solve artificial intelligence (AI) problems, a RNN, still composed of Neural Networks, can remember them because they are recurrent but encounters some limits. This is why the LSTM model appears to be more effective within the scope of text generation \cite{chollet_deep_2018} \cite{raiman_nano_2015} \cite{surma_text_2018} : it has the possibility to correct the vanishing gradient problem of the RNN and thus learning what to remember and what to forget. An interesting work was made on the use of ghostwriting through LSTM  \cite{potash_ghostwriter:_2015} : the goal is to give the impression that a rapper has produced a new song, by reproducing his style of writing. \newline

It is necessary to base our generation on a pre-existing set of lyrics from one or several artists on which we train our model. We also considered the differents ways of predicting the next character, the next word or the next line. As they are several ways of doing so, we have shown some interest in the way Ruslan Nikolaev \cite{nikolaev_generating_2018} \cite{nikolaev_drake-lyric-generator_2018} showed the effectiveness of the character-level model in 2018. Also, Hirjee and Brown have created a rhyme detection tool based on probabilistic model.\cite{hirjee_using_2010} \newline

Still, methods to estimate the quality of generated lyrics are noted by Potash, Romanov and Rumshisky in 2015 and Malmi and his colleagues in 2016 \cite{malmi_dopelearning:_2016}, considering the style of the rapper and the rhyme density. Still, the text has to answer to some considerations such as the occurrence of linguistic phenomena as lexis and phonetics, even if most of the time, the program elaborated appears to give a “word salad”   \cite{oliveira_automatic_nodate} \cite{paupier_how_2018}. \newline

This is why using the CMU Pronouncing Dictionary appears to be essential \cite{hirjee_using_2010}, as it make it possible for us to capture the global meaning of a group of words. One of the goal of this research is to have a nicely generated pronunciation at the same time. Finally, Malmi in 2016 also considers the length of verses in a rap text generated and all the kind of rhymes that we can find in it. This work, then, will be a mixture of all of these previous works, trying to produce a more precise model for rap lyrics generation, including considerations on metric and length, phonemes, question of predictions and judgment on the uniqueness of the production. Then, we will intend to present a study of a generated ghostswritten text based on some Kayne West's lyrics, considering the accentuation on syllables of this text, the metric and the rhymes having yet partially been studied, as the lexical aspect, another of our goal being to produce a coherent new rap text, being as coherent as the Kayne West's ones. \newline

\section{Data and Methods}

One of the main exercices we had to perfom was writing a model in Python, using the TensorFlow library and a RNN, that would take into account our needs. This program is available for free, under the GPLv3 licence, and is available with this article. In it, we have added the CMU Pronouncing Dictionnary, allowing us to read the generated text produced with the good accents, considering the word's syllable(s) and the metric of the setence, the lexical stress playing also a role in the accentuation of words. Moreover, a code allowing us to recognize the rhyming syllables at the and of lines was introduced. The intial set of lyrics came from Rap Genius, where we downloaded the lyrics thanks to a scraper \cite{paupier_raplyrics-scraper_2018}, hoping to produce ghostwritten texts, as if Kayne West had produced a new song. In this way, we calculated the similarity between the verses generated and Kanye West's verses, in order to see if our generated lyrics were the"freshest" plus a calculation of syllables density and lines length enabling us to see if the produced line resemble to the artist’s one, or is even better than what has produced Kanye West. Then, we trained the model on a computer with a nVidia© GTX 760 GPU card, 12Gb of RAM, and a Intel©  i5-3570K CPU @ 3.40GHz for about 113.000 iterations. The training took one afternoon, while using the computer at the same time to analyse the results. You can see a example of the evolution through the iterations in the poster accompagning our research, as well as an example of generated lyrics. \newline

\section{Results and Discussion}

In the generated texts, we can observe a important quantity of nonsensical words, as in the iteration n°1000 : "kidsiin", "throuictifing", "griends" for examples. Then, the CMU Pronouncing Dictionnary was not really useful for their pronunciation because it did not recognize these words, being non listed. Other words also were not recognized by the Dictionnary, because they were shortened, mixed (ex : "nigga", "tryna"; it is really usual in K. W. texts) or not well spelled (ex : "ocident"). On the whole, the first generated lines had not a lot of sens and the rate of similarity with Kanye West's produced was really low, considering the loss of the generated lines. The more we trained our model, the more the produced text were meaningful. The final iteration completely illustrate this fact : "Now throw ya hands up hustlers, busters, boosters, hoes

[Verse 4: Kanye West]
Strap kicked at me Bustack
Loyee oyee or suits of the leaves
Yeah I used to lose helped

[Verse 3: Desiigner]
I got a plan, I spit this jeople for Beans of y'all
Lookin' at my own money, while y'all niggas packs
You know the shit reasong, ah school bust you can't connect
When you gon' end up up"
We still have nonsensical words as "loyee oyee" (interjections in K. W. are also common), "reasong" or shortened words as "niggas", "lookin'", etc. But we can clearly imagine the first line been rapped by a rapper. Also, we can note that there is more regularity in the lines length than for the previous iterations. Considering this rate, even if the wole generated text has in general is a bit shorter than the lines of Kanye, the length still remain close from the original lins length (the average for both of them turns around 4,1 - 4,3 words). We even have inidications on who is rapping. This is why the more we train our program, the more the rate of similarity is close from 0, meaning that between the cursus of verses from K. West and our cursus of verses, there is fewer and fewer loss. Because of the non-identified words problem, we may have the feeling that the lines in the generated text contain less syllables the the original one. Nevertheless, we observe that the general average of syllables in the generated lines turns around 8 syllables per line while the one of the original text turns around 9.3 syllables per line : they are still very close. Then, it could be interesting to integrate in this kind of program a dictionnary that recognizes not the word as a whole (which is important for the pronunciation of the generated text) but first every syllables of every word, even if it has not sens or countains mistakes in its spelling. It may also be interseting to create a program with rhyming constraints.

\bibliography{References}
\bibliographystyle{ieeetr}

\end{document}
